[{"authors":null,"categories":null,"content":"I am a PhD Student at Potsdam Institute for Climate Impact Research in the Future Lab Social Metabolism and Impacts and at Humboldt University of Berlin. I am studying the relationship between energy use, population growth and sustainability in low-income countries. My research lies at the intersection of different fields: energy sociology, demography, social metabolism and gender studies. I am working with survey data, statistics, micro-simulation modelling, mostly in R.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://camillebelmin.github.io/author/camille-belmin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/camille-belmin/","section":"authors","summary":"I am a PhD Student at Potsdam Institute for Climate Impact Research in the Future Lab Social Metabolism and Impacts and at Humboldt University of Berlin. I am studying the relationship between energy use, population growth and sustainability in low-income countries.","tags":null,"title":"Camille Belmin","type":"authors"},{"authors":[],"categories":null,"content":"Seminar on Gender and Energy organized by Women in Environmental Economics for Development (WinEED). The recorded zoom presentation and the slides can be seen here.\n","date":1606089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606089600,"objectID":"f4130e74f6b7d1c7c9ed592bfc0c03e5","permalink":"https://camillebelmin.github.io/talk/wineed-seminar/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/wineed-seminar/","section":"talk","summary":"Seminar on Gender and Energy organized by Women in Environmental Economics for Development","tags":[],"title":"WinEED Seminar","type":"talk"},{"authors":null,"categories":null,"content":"Last week I submitted my first article as part of my PhD. This paper was written in R markdown and is fully reproducible. It means that a single document contains all the manuscript but also all the code for the analysis and figure generation. All the pre-processings are also reproducible and the code is available on my gitlab. I am pretty happy to contribute to reproducible science, and I hope that my code is user-friendly enough to enable other people using it. Truth is, it was not the most straightforward way to write the paper, and consumed a lot of time. It does not mean it wasn\u0026rsquo;t worth it, and that I would not do it again. However, I would like to improve this reproducible workflow and learn from other people\u0026rsquo;s experience. That is why I am writing this short post to talk about the workflow we adopted and the challenges associated with this workflow, and start to think about how we could do differently in the future.\n  The workflow adopted to write our reproducible paper, with a lot of back and forth from Word/Open Office to R markdown   What is the workflow we adopted? On the figure above, I schematized the workflow we adopted for writing this paper. First, we iterated only on a word document (step 1) to be able to track the changes made by my co-authors. This allowed to have my sentences corrected and to improve my writing. After the draft was relatively mature, I created a R markdown (Rmd) file using the text we had come up with, together with all the code for the analysis and figure generation (step 2). From this point, the internal reviewing process worked as follows: I knited the Rmd to Word (step 3), then my co-authors would successively add changes and comment the draft (step 4). When they were all done, I then manually included their changes back in the Rmd file, mostly by overusing the Ctrl + C and Ctrl + V commands (step 3). This was very time consuming. For each edit: I spotted the edit in word, copied the sentence, then I searched the exact location of the corresponding text in R markdown (Screenshot below), then paste it there. Then, in order not to loose track of the changes I already had included, I accepted the changes in the word document.\n  The process of integrating changes from a word document with track change to R markdown   I repeated that for each changes, and for each the rounds of review (i.e. a lot of times, and a lot of headaches ü§Ø).\n  It was probably not the most intelligent/efficient way of proceeding, but once launched in the process, it was just easier to continue like this.\nWhat are alternative workflows?   If track change is an absolute necessity (I tend to think so), I think the most obvious alternative is to use Word for the whole process, and to switch to R markdown at the end. However, it is hard to know when is actually going to be the last round of edit and getting used to the Rmarkdown-word process (steps 3-4) can take some time. If the paper needs to be submitted before a deadline, this could become quite stressful.\n  Another solution is that co-authors add changes directly on the Rmd. This requires that all co-authors are comfortable with navigating in a Rmd document, with potentially a lot of code chunks. Rstudio will soon release [Visual Rstudio](https://blog.rstudio.com/2020/09/30/rstudio-v1-4-preview-visual-markdown-editing/), that provides a user friendly visual text editor and this will hopefully attract more people to R markdown (and reproducibility üòä!). For example, users can now see their content change in real-time as they write. It also highlights spelling mistakes as in Word/Libre Office writer, really nice! You can try the preview [here](https://rstudio.com/products/rstudio/download/preview/). Unfortunately, track change or comments are still not available in this version. It makes it difficult to keep track of changes in collaborative work. One solution would be to commit to a git repository and visualizing the difference between two files, but this seems like an overly complicated process.\n  Using one Word document to write the paper, and have a separate Rmd file containing all the analysis and figure generation in a Rmd file. Then, whenever the numbers/tables/figures in the text needed to be updated, I would include them manually in the Word file.\n  I am aware that some R packages are being developed like redoc and reviewer. Unfortunately, the development of the former has now been suspended. Regarding reviewer, I have not tried it. My feeling is that people reluctant to using R or R markdown would have difficulty engaging with the method they propose. If you have a good experience with it, I would be happy to hear your feedback.\n  However I find that all of these workflow are imperfect for me, and I am still looking in search\u0026hellip; If you have alternative solutions, please let me know!\nBottom line The tools used to write a reproducible quantitative paper depend on your co-authors' respective tastes and their familiarity with available tools. The best workflow for you is not necessarily the best for your colleagues, and the other way around. One has to adapt and find a common ground, and also probably experience different workflows to see what fits best, under which circumstances. I am still looking for the perfect reproducible workflow, given the constrains that most of my colleagues are not necessarily familiar with writing in R markdown, and that I value track changes as part of the writing process.\nWhat about you? I would be interested to hear about experience from people who have written reproducible papers. What was your workflow? Was it efficient? What were your constraints? Would you recommend it? Please share your story! ‚ò∫Ô∏è\n","date":1600214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600214400,"objectID":"305032a1289273f5cd86a0a82ea20261","permalink":"https://camillebelmin.github.io/post/reproducible-paper-writing/","publishdate":"2020-09-16T00:00:00Z","relpermalink":"/post/reproducible-paper-writing/","section":"post","summary":"Last week I submitted my first article as part of my PhD. This paper was written in R markdown and is fully reproducible. It means that a single document contains all the manuscript but also all the code for the analysis and figure generation.","tags":null,"title":"Wanted: a perfect reproducible paper workflow","type":"post"},{"authors":null,"categories":null,"content":"In this post, I present a dataset that I created, containing the harmonized district names and regions of 254 demographic and health surveys (DHS). I explain why I created it, how I did it and how to use the dataset.\n1. The need for an harmonization of DHS districts The DHS data are micro-datasets monitored by the the DHS Program (ICF 2017) (mainly USAID funded) since 1984. They cover more than 90 countries and 400 surveys. The DHS dataset is a rich source of data for a large set of research topics such as health, epidemiology, economy, demography or sociology. Local authorities are responsible for implementing the survey in their own countries but the methodology of DHS surveys is standardized, which makes them even more useful when it comes to conducting cross-country analyses. However, differences in the encoding of variables can still occur. In particular, the reporting of region/district names is far from being standardized. This can be due to differences in reporting languages or simply typos. For example, a given region of a country can be reported ‚ÄúCenter‚Äù (US English) with the index 5 in year 1990, and ‚ÄúCentre‚Äù (Brit., French) with the index 3 in 1995. Another type of change in region index and names reporting can be due to a political change of administrative sub-divisions (merging/splitting of regions or complete re-definition of administrative sub-divisions). These discontinuities can be a problem when one wants to create panel data set for one or several countries, the unit of the panel being the region/district. For my PhD project, I needed to build a panel data set for the district level for as many DHS surveys (and therefore countries) as I could. I therefore spent some time harmonizing the region names and indices. I decided to share this dataset that I created and to explain in this document how I built it and how to use it.\n2. How did I create the dataset? In order to create this dataset, I first loaded all the DHS datasets and kept only the information on DHS survey ID, region name and region index and I put it together in one single file. I then used four different types of methods of harmonization, and added a column \u0026ldquo;method_harmonization\u0026rdquo; to tell which method was used for each DHS survey. The four methods are the following:\n1- The most simple (but laborious) thing to do was to correct typos and wrongly encoded region indices. For example, a region could have been encoded ‚ÄúATLANTique‚Äù in 2000 and ‚ÄúAtlantique‚Äù in 2005, or a difference in the language could occur (for example North (English) and Nord (French)).\n3- When no information was available from the IPUMS-DHS harmonization, I used the DHS spatial repository, which mapped the boundaries of sub-national regions in DHS surveys, over time. It allows to visualize change, merge or division of regions over time. Some specific notes on this harmonization can be found on the document \u0026ldquo;notes_harmonization.md\u0026rdquo;.\n4- For a few surveys, I did the harmonization myself based on my own research. For example, Uganda 2016, the regions of North Buganda and Souhth Buganda were put in the region Central 1 + Central 2 + Kampala because the Central region is coterminous with the Kingdom of Buganda, one of the ancient African monarchies that are constitutionally recognised in Uganda Source. Further notes are available on the document \u0026ldquo;notes_harmonization.md\u0026rdquo;.\nSome surveys where removed in the process:\n  The countries that had only one wave of DHS survey because there was no cause for harmonization.\n  Some surveys for which there was no harmonization possible. For example when a country has two waves with very different administrative sub-divisions, I didn‚Äôt know how to harmonize.\n  Some specific countries for which the number of region varies too much across the waves (for example Dominican Republic)\n  For the region name harmonization, I created two columns:\n  region_name_clean: here, I only corrected the typos, the change of language of the region (i.e.¬†Nord to North). I did not merge any regions in this column.\n  region_name_harmonized: this column has cleaned column names and also harmonized regions by sometimes changing the administrative borders (following what the IPUMS-DHS).\n  3. The dataset The dataset can be found on my git repository here. Here is an extract of the dataset, for the country Benin, the year 2012 and 2017:\n  country_name\n survey_year\n survey_id\n region_num_raw\n region_num_harmonized\n region_name_raw\n region_name_clean\n region_name_harmonized\n     Benin\n 2017\n BJ2017DHS\n 1\n 3\n ALIBORI\n Alibori\n Borgou + Alibori\n   Benin\n 2017\n BJ2017DHS\n 2\n 1\n ATACORA\n Atacora\n Atacora + Donga\n   Benin\n 2017\n BJ2017DHS\n 3\n 2\n ATLANTic\n Atlantique\n Atlantique + Littoral\n   Benin\n 2017\n BJ2017DHS\n 4\n 3\n BORGOU\n Borgou\n Borgou + Alibori\n   Benin\n 2017\n BJ2017DHS\n 5\n 6\n COLLINES\n Collines\n Zou + Collines\n   Benin\n 2017\n BJ2017DHS\n 6\n 4\n COUFFO\n Couffo\n Mono + Couffo\n   Benin\n 2017\n BJ2017DHS\n 7\n 1\n DONGA\n Donga\n Atacora + Donga\n   Benin\n 2017\n BJ2017DHS\n 8\n 2\n LITTORAL\n Littoral\n Atlantique + Littoral\n   Benin\n 2017\n BJ2017DHS\n 9\n 4\n MONO\n Mono\n Mono + Couffo\n   Benin\n 2017\n BJ2017DHS\n 10\n 5\n OU√âM√â\n Oueme\n Oueme + Plateau\n   Benin\n 2017\n BJ2017DHS\n 11\n 5\n PLATEAU\n Plateau\n Oueme + Plateau\n   Benin\n 2017\n BJ2017DHS\n 12\n 6\n ZOU\n Zou\n Zou + Collines\n    Here is the description of the columns of the dataset:\n   Column\n Description\n Note\n     country_name\n Name of the country of the DHS survey\n    country_code\n Country code of the country\n alpha-3 code\n   survey_year\n Year of the DHS survey\n    file_name\n File name of the survey I used for the harmonization\n One survey has different file depending on the recoding. I used the Individual recode with SPSS file format (.sav) to create this dataset\n   survey_id\n Unique ID of the DHS survey\n    region_num_raw\n Index of the DHS region as stated originally in the dataset\n    region_num_harmonized\n Harmonized index of the DHS region\n    region_name_raw\n Name of the DHS region as stated originally in the dataset\n    region_name_clean\n Name of the DHS region as stated originally in the dataset, but without typos, language differences, and special characters\n    region_name_harmonized\n Harmonized name of the DHS region. Typos were removed, language of region harmonized, and special characters removed. In addition, the administrative borders have been harmonized (i.e merged when they were splited) from a year to another, thanks to the IPUMS-DHS region harmonization.\n     4. How to use it? When you are working on a study involving several waves of DHS of a country in which you need to aggregate the data at the sub-national level, here is how to do to harmonize the region names and index using my concordance dataset. The code below works with the package rdhs, which is a package for the management and analysis of DHS data. To make this code work, you‚Äôll have to download the DHS data through the rdhs package. All the steps to do so are described here (in ‚Äú3. Download survey datasets‚Äù). (To ask for access to DHS data ‚Äì\u0026gt; go to the DHS Program website)\nHere are the steps to use my concordance dataset to harmonize the regions names of the surveys in which you are interested:\n1- First of all, download the git repository, and check in the dhs_harmonization_dataset.csv if the DHS surveys in which you are interested have been harmonized.\n2- Load the meta-information on the DHS surveys for which you have access granted: (For information, the correspondance between DHS country code and country names can be found here)\nlibrary(dplyr) library(reactable) # Set the path of the git repository and of data folder path_git_repo = getwd() path_toy_dhs_data \u0026lt;- paste0(path_git_repo, \u0026quot;/DHS_region_harmonization_files/toy_DHS_data\u0026quot;) # Select the country code of the country on which you which to work country_code \u0026lt;- \u0026quot;BJ\u0026quot; # In this example we will mimic to use Benin data (although I am only using toy data) # If you have access to the DHS data, Do this step: ## Load the dataset with information on all the dhs surveys for which you have access granted # library(rdhs) # info_dhs_datasets \u0026lt;- # rdhs::dhs_datasets(fileFormat = \u0026quot;spss\u0026quot;, fileType = \u0026quot;IR\u0026quot;, surveyType = \u0026quot;DHS\u0026quot;) %\u0026gt;% #dplyr::select(SurveyId, SurveyYear, DHS_CountryCode, CountryName, FileName) %\u0026gt;% #rename(zip_file_name = FileName) %\u0026gt;% # mutate(rds_file_name = # create a column corresponding to the RDS file name instead of zip file name # paste0(substr(zip_file_name, start=1, stop = 8 ),\u0026quot;.rds\u0026quot;)) # If you don't have access to the dhs data, run this instead: info_dhs_datasets \u0026lt;- read_csv(file = paste0(path_git_repo, \u0026quot;/DHS_region_harmonization_files/info_dhs_dataset.csv\u0026quot;)) ## Parsed with column specification: ## cols( ## SurveyId = col_character(), ## SurveyYear = col_double(), ## DHS_CountryCode = col_character(), ## CountryName = col_character(), ## zip_file_name = col_character(), ## rds_file_name = col_character() ## ) # Dataframe the infos on only the surveys you are interested in file_names_one_country \u0026lt;- info_dhs_datasets %\u0026gt;% dplyr::filter(DHS_CountryCode == country_code) %\u0026gt;% dplyr::select(rds_file_name, SurveyYear, SurveyId)  3 - Load all of the survey datasets you need and select your variables of interest. Make sure your selection includes the variable region (v024 if the survey is after 1989, v101 otherwise). Again, I recommend to use the package rdhs to be able to load your DHS data in the RDS format. Please note that in this document I am using toy data and not the real DHS dataset of Benin.\n# Load the dhs survey with RDS country_dhs_survey_data \u0026lt;- data.frame() for(i in 1:nrow(file_names_one_country)) { # this loops over all the DHS surveys available for ONE country. current_dhs_survey_data \u0026lt;- readRDS(file =paste0(path_toy_dhs_data, \u0026quot;/\u0026quot;, file_names_one_country$rds_file_name[i])) %\u0026gt;% dplyr::select( v024, # variable name of the DHS district /!\\ if the survey is from 1989 or before the region name is v101, not v024 v119, v106, v012) %\u0026gt;% # select your variable of interest. Here: electricity, highest education level and age dplyr::rename(region_num_raw = v024) %\u0026gt;% mutate(survey_id = file_names_one_country$SurveyId[i]) # add the corresponding survey id region_labels \u0026lt;- stack(attr(current_dhs_survey_data$region_num_raw, 'labels')) %\u0026gt;% # get labels and index correspondance as.data.frame() %\u0026gt;% rename(region_num_raw = values, # rename them to match it the DHS region harmonization dataset region_name_raw = ind) # Join the region_labels dataframe with our dhs surveys, to get the raw region name current_dhs_survey_data \u0026lt;- current_dhs_survey_data %\u0026gt;% left_join(region_labels, by = \u0026quot;region_num_raw\u0026quot;) current_dhs_survey_data$region_num_raw \u0026lt;- as.numeric(as.character(current_dhs_survey_data$region_num_raw)) country_dhs_survey_data \u0026lt;- rbind(country_dhs_survey_data, # bind the current dhs data to previous ones current_dhs_survey_data) }  Here is what the DHS data look like BEFORE the region harmonization, for only one region of Benin 2017:\n  country_name\n survey_year\n survey_id\n region_num_raw\n region_name_raw\n v119\n v106\n v012\n     Benin\n 2017\n BJ2017DHS\n 3\n ATLANTic\n 1\n 2\n 23\n   Benin\n 2017\n BJ2017DHS\n 3\n ATLANTic\n 0\n 3\n 37\n   Benin\n 2017\n BJ2017DHS\n 3\n ATLANTic\n 0\n 0\n 38\n   Benin\n 2017\n BJ2017DHS\n 3\n ATLANTic\n 0\n 0\n 31\n   Benin\n 2017\n BJ2017DHS\n 3\n ATLANTic\n 0\n 0\n 40\n   Benin\n 2017\n BJ2017DHS\n 3\n ATLANTic\n 0\n 1\n 32\n    4- Join the DHS surveys you are interested in with the dhs_harmonized_region_name dataset.\n# Then join your dhs surveys with the dhs_harmonized_region_name dataset country_dhs_survey_data \u0026lt;- country_dhs_survey_data %\u0026gt;% select(-region_name_raw, -country_name, -survey_year) %\u0026gt;% left_join(dhs_region_harmo, by = c(\u0026quot;survey_id\u0026quot;, \u0026quot;region_num_raw\u0026quot;)) # Re-order the columns country_dhs_survey_data \u0026lt;- cbind(country_dhs_survey_data %\u0026gt;% select(country_name, country_code, survey_year, survey_id, file_name, region_num_raw,region_num_harmonized, region_name_raw, region_name_clean, region_name_harmonized), country_dhs_survey_data %\u0026gt;% select(-country_name, -country_code, -survey_year, -survey_id, -file_name, -region_num_raw, - region_num_harmonized, -region_name_raw, -region_name_clean, -region_name_harmonized))  Here is what the DHS data look like AFTER the region harmonization, for only one region of Benin 2017:\n  country_name\n survey_year\n survey_id\n region_num_raw\n region_num_harmonized\n region_name_raw\n region_name_clean\n region_name_harmonized\n v119\n v106\n v012\n     Benin\n 2017\n BJ2017DHS\n 3\n 2\n ATLANTic\n Atlantique\n Atlantique + Littoral\n 1\n 2\n 23\n   Benin\n 2017\n BJ2017DHS\n 3\n 2\n ATLANTic\n Atlantique\n Atlantique + Littoral\n 0\n 3\n 37\n   Benin\n 2017\n BJ2017DHS\n 3\n 2\n ATLANTic\n Atlantique\n Atlantique + Littoral\n 0\n 0\n 38\n   Benin\n 2017\n BJ2017DHS\n 3\n 2\n ATLANTic\n Atlantique\n Atlantique + Littoral\n 0\n 0\n 31\n   Benin\n 2017\n BJ2017DHS\n 3\n 2\n ATLANTic\n Atlantique\n Atlantique + Littoral\n 0\n 0\n 40\n   Benin\n 2017\n BJ2017DHS\n 3\n 2\n ATLANTic\n Atlantique\n Atlantique + Littoral\n 0\n 1\n 32\n    There you go! You now have all the DHS datasets in which you are interested with their region name harmonized from one year to another. This allows you for example to create a panel dataset easily: you‚Äôll just have to aggregate the data at the regional (i.e.¬†district) level, and you will have repeated measures over time!\nDisclaimer: This dataset is ‚Äúhome made‚Äù and I do not guarantee it is flawless. If you have any question, report of errors, please contact me or make a pull request!\nLicensing for the dataset\n \nReferences Elizabeth Heger Boyle, Miriam King, and Matthew Sobek. 2019. ‚ÄúIPUMS-Demographic and Health Surveys: Version 7 [Dataset].‚Äù The Journal of Development Studies. Minnesota Population Center; ICF International, 2019. https://doi.org/https://doi.org/10.18128/D080.V7.\n ICF, Funded by USAID. 2017. ‚ÄúDemographic and Health Surveys (Various) [Datasets].‚Äù https://doi.org/https://doi.org/10.18128/D080.V7.\n  ","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"9fe05a96c017dd969d8bc17035f58256","permalink":"https://camillebelmin.github.io/post/dhs-region-harmonization/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/post/dhs-region-harmonization/","section":"post","summary":"In this post, I present a dataset that I created, containing the harmonized district names and regions of 254 demographic and health surveys (DHS). I explain why I created it, how I did it and how to use the dataset.","tags":null,"title":"A dataset of harmonized DHS districts","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://camillebelmin.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]